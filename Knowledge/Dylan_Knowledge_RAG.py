# -*- coding: utf-8 -*-
"""AgentSpace Knowledge RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CAdpnM5tuN7MMx6mNTO4y-Sjy2X20lwF
"""

import os

os.environ["NEO4J_URI"] = "neo4j+s://7297c836.databases.neo4j.io"
os.environ["NEO4J_USERNAME"] = "neo4j"
os.environ["NEO4J_PASSWORD"] = "6X6ZtF27S5Mt6-nuWQ4MjDGXhQI7UJaglEAfA_VbBQI"
os.environ["OPENAI_API_KEY"] = "yourkey"

!pip install pypdf

from langchain_community.document_loaders import PyPDFLoader
import glob

docs = []

for file in glob.glob("/content/*.pdf"):
    loader = PyPDFLoader(file)
    docs.extend(loader.load())

len(docs)

from langchain_text_splitters import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=100
)

chunks = splitter.split_documents(docs)
len(chunks)

!pip install huggingface_hub --quiet
from huggingface_hub import login

login()

from sentence_transformers import SentenceTransformer
from langchain.embeddings.base import Embeddings

model = SentenceTransformer("google/embeddinggemma-300m")

class GemmaEmbeddings(Embeddings):
    def embed_documents(self, texts):
        return model.encode(texts, convert_to_numpy=True).tolist()

    def embed_query(self, text):
        return model.encode([text], convert_to_numpy=True).tolist()

embeddings = GemmaEmbeddings()

import numpy as np

class GemmaEmbeddingsNormalized(GemmaEmbeddings):
    def embed_documents(self, texts):
        vectors = super().embed_documents(texts)
        vectors = [v/np.linalg.norm(v) if np.linalg.norm(v) > 0 else np.zeros_like(v) for v in vectors]
        return vectors

    def embed_query(self, text):
        vector = super().embed_query(text)[0]
        return vector/np.linalg.norm(vector) if np.linalg.norm(vector) > 0 else np.zeros_like(vector)
embeddings = GemmaEmbeddingsNormalized()

for idx, chunk in enumerate(chunks):
    chunk.metadata['chunk_id'] = idx

!pip install neo4j

from langchain_community.vectorstores import Neo4jVector

vectorstore = Neo4jVector.from_documents(
    documents=chunks,
    embedding=embeddings,
    url=os.environ["NEO4J_URI"],
    username=os.environ["NEO4J_USERNAME"],
    password=os.environ["NEO4J_PASSWORD"],
    index_name="essay_chunkAgentSpace2",
    node_label="Chunk",
    text_node_property="text",
    embedding_node_property="embedding"
)

from langchain_community.graphs import Neo4jGraph

graph = Neo4jGraph(
    url=os.environ["NEO4J_URI"],
    username=os.environ["NEO4J_USERNAME"],
    password=os.environ["NEO4J_PASSWORD"]
)

chunks_by_source = {}
for chunk in chunks:
    source = chunk.metadata.get('source', 'unknown')
    if source not in chunks_by_source:
        chunks_by_source[source] = []
    chunks_by_source[source].append(chunk)

print("Creating NEXT relationships...")
relationship_count = 0

for source, source_chunks in chunks_by_source.items():
    source_chunks.sort(key=lambda x: x.metadata.get('chunk_id', 0))

    for i in range(len(source_chunks) - 1):
        current_id = source_chunks[i].metadata['chunk_id']
        next_id = source_chunks[i + 1].metadata['chunk_id']

        query = """
        MATCH (c1:Chunk)
        WHERE c1.chunk_id = $current_id
        MATCH (c2:Chunk)
        WHERE c2.chunk_id = $next_id
        MERGE (c1)-[:NEXT]->(c2)
        """

        graph.query(query, params={
            "current_id": current_id,
            "next_id": next_id
        })
        relationship_count += 1

print(f"Created {relationship_count} NEXT relationships")

print("Creating Document nodes...")

for source, source_chunks in chunks_by_source.items():
    doc_query = """
    MERGE (d:Document {name: $source})
    SET d.chunk_count = $chunk_count
    """

    graph.query(doc_query, params={
        "source": source,
        "chunk_count": len(source_chunks)
    })

    for chunk in source_chunks:
        link_query = """
        MATCH (d:Document {name: $source})
        MATCH (c:Chunk)
        WHERE c.chunk_id = $chunk_id
        MERGE (c)-[:PART_OF]->(d)
        """
        graph.query(link_query, params={
            "source": source,
            "chunk_id": chunk.metadata['chunk_id']
        })

print(f"Created {len(chunks_by_source)} Document nodes")

def get_chunk_context(chunk_id, context_size=1):
    """Get neighboring chunks using NEXT relationships"""
    query = """
    MATCH (c:Chunk)
    WHERE c.chunk_id = $chunk_id
    OPTIONAL MATCH path_before = (before:Chunk)-[:NEXT*1..%d]->(c)
    OPTIONAL MATCH path_after = (c)-[:NEXT*1..%d]->(after:Chunk)
    WITH c,
         collect(DISTINCT before) as before_chunks,
         collect(DISTINCT after) as after_chunks
    RETURN
        c.text as current_text,
        c.chunk_id as current_id,
        [b in before_chunks | b.text] as before_texts,
        [a in after_chunks | a.text] as after_texts
    """ % (context_size, context_size)

    result = graph.query(query, params={"chunk_id": chunk_id})
    return result[0] if result else None

def hybrid_search(query_text, k=4, context_size=1):
    """Combine vector search with graph context"""
    results = vectorstore.similarity_search_with_score(query_text, k=k)

    hybrid_results = []
    for doc, score in results:
        chunk_id = doc.metadata.get('chunk_id')

        context = get_chunk_context(chunk_id, context_size) if chunk_id is not None else None

        hybrid_results.append({
            'text': doc.page_content,
            'score': score,
            'chunk_id': chunk_id,
            'source': doc.metadata.get('source', 'unknown'),
            'context': context
        })

    return hybrid_results

retriever = vectorstore.as_retriever(search_kwargs={"k": 4})

query = "What specific argument does the author make about orchids"
results = retriever.invoke(query)

for i, r in enumerate(results):
    print(f"---- Result {i+1} ----")
    print(r.page_content[:300])

print("\n\n=== HYBRID SEARCH WITH CONTEXT ===")
hybrid_results = hybrid_search(query, k=4, context_size=1)

for i, result in enumerate(hybrid_results, 1):
    print(f"\n---- Hybrid Result {i} ----")
    print(f"Score: {result['score']:.4f}")
    print(f"Source: {result['source']}")
    print(f"Text: {result['text'][:200]}...")

    if result['context']:
        ctx = result['context']
        print(f"Context: {len(ctx.get('before_texts', []))} before, {len(ctx.get('after_texts', []))} after chunks")