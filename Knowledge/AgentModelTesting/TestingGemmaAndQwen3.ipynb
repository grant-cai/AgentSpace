{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcf5665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8b674001",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting environment variables\n",
    "# Load environment variables from .env file\n",
    "load_dotenv('../.env')\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = os.getenv('LANGSMITH_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b601400b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "os.environ[\"GEMINI_API_KEY\"] = os.getenv('GEMMA_KEY')\n",
    "\n",
    "model = init_chat_model(\n",
    "    \"gemma-3-12b-it\", # Use the specific model version you need\n",
    "    model_provider=\"google_genai\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b6f0a9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess documents \n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\",\n",
    "    \"https://lilianweng.github.io/posts/2024-07-07-hallucination/\",\n",
    "    \"https://lilianweng.github.io/posts/2024-04-12-diffusion-video/\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bf8a0599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Reward Hacking in Reinforcement Learning | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Reward Hacking in Reinforcement Learning\\n    \\nDate: November 28, 2024  |  Estimated Reading Time: 37 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nBackground\\n\\nReward Function in RL\\n\\nSpurious Correlation\\n\\n\\nLet‚Äôs Define Reward Hacking\\n\\nList of Examples\\n\\nReward hacking examples in RL tasks\\n\\nReward hacking examples in LLM tasks\\n\\nReward hacking examples in real life\\n\\n\\nWhy does Reward Hacking Exist?\\n\\n\\nHacking RL Environment\\n\\nHacking RLHF of LLMs\\n\\nHacking the Training Process\\n\\nHacking the Evaluator\\n\\nIn-Context Reward Hacking\\n\\n\\nGeneralization of Hacking Skills\\n\\nPeek into Mitigations\\n\\nRL Algorithm Improvement\\n\\nDetecting Reward Hacking\\n\\nData Analysis of RLHF\\n\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nReward hacking occurs when a reinforcement learning (RL) agent exploits flaws or ambiguities in the reward function to ac\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0][0].page_content.strip()[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9429113c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split fetched documents into smaller chunks for vector store \n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=100, chunk_overlap=50\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "533389d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'}\n",
    ")\n",
    "\n",
    "# 2. Initialize the Vector Store with the embedding model\n",
    "vectorstore = InMemoryVectorStore(embeddings)\n",
    "\n",
    "# 3. Add your documents\n",
    "# This step creates the embeddings and stores them in memory\n",
    "vectorstore.add_documents(doc_splits)\n",
    "\n",
    "# 4. Create your retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a0e34504",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a prebuilt retriever tool. The classic library they have written is now more deprecated and old\n",
    "from langchain_core.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retrieve_blog_posts\",\n",
    "    \"Search and return information about Lilian Weng blog posts.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "71259a31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why does Reward Hacking Exist?#\\n\\nReward hacking examples in LLM tasks\\n\\nReward hacking examples in real life\\n\\n\\nWhy does Reward Hacking Exist?\\n\\n\\nHacking RL Environment\\n\\nHacking RLHF of LLMs\\n\\nHacking the Training Process\\n\\nHacking the Evaluator\\n\\nIn-Context Reward Hacking\\n\\n\\nGeneralization of Hacking Skills\\n\\nPeek into Mitigations\\n\\nRL Algorithm Improvement\\n\\nDetecting Reward Hacking\\n\\nMost of the past work on this topic has been quite theoretical and focused on defining or demonstrating the existence of reward hacking. However, research into practical mitigations, especially in the context of RLHF and LLMs, remains limited. I especially want to call out for more research efforts directed toward understanding and developing mitigation for reward hacking in the future. Hope I will be able to cover the mitigation part in a dedicated post soon.\\nBackground#\\nReward Function in RL#'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test the tool\n",
    "retriever_tool.invoke({\"query\": \"types of reward hacking\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bb0ef2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build generate query node\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "#Build generate query node\n",
    "from langgraph.graph import MessagesState\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "os.environ[\"QWEN3_API_KEY\"] = os.getenv('QWEN3_KEY')\n",
    "\n",
    "response_model = ChatOpenAI(\n",
    "    model=\"qwen3\",  # Specify a model available on OpenRouter\n",
    "    api_key=os.environ[\"QWEN3_API_KEY\"],\n",
    "    base_url=\"https://ellm.nrp-nautilus.io/v1\",\n",
    ")\n",
    "\n",
    "def generate_query_or_respond(state: MessagesState):\n",
    "    \"\"\"Call the model to generate a response based on the current state. Given\n",
    "    the question, it will decide to retrieve using the retriever tool, or simply respond to the user.\n",
    "    \"\"\"\n",
    "    response = (\n",
    "        response_model\n",
    "        .bind_tools([retriever_tool]).invoke(state[\"messages\"])  \n",
    "    )\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3afc1830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "\n",
      "\n",
      "Hello! How can I assist you today? üòä\n"
     ]
    }
   ],
   "source": [
    "input = {\"messages\": [{\"role\": \"user\", \"content\": \"hello!\"}]}\n",
    "generate_query_or_respond(input)[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "81a1c8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve_blog_posts (chatcmpl-tool-82cbe7c1358beb3f)\n",
      " Call ID: chatcmpl-tool-82cbe7c1358beb3f\n",
      "  Args:\n",
      "    query: types of reward hacking\n"
     ]
    }
   ],
   "source": [
    "input = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What does Lilian Weng say about types of reward hacking?\",\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "generate_query_or_respond(input)[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c0517651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rewrite_question'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "GRADE_PROMPT = (\n",
    "    \"You are a grader assessing relevance of a retrieved document to a user question. \\n \"\n",
    "    \"Here is the retrieved document: \\n\\n {context} \\n\\n\"\n",
    "    \"Here is the user question: {question} \\n\"\n",
    "    \"If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\"\n",
    "    \"Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\n",
    ")\n",
    "\n",
    "\n",
    "class GradeDocuments(BaseModel):  \n",
    "    \"\"\"Grade documents using a binary score for relevance check.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Relevance score: 'yes' if relevant, or 'no' if not relevant\"\n",
    "    )\n",
    "\n",
    "\n",
    "grader_model = ChatOpenAI(\n",
    "    model=\"qwen3\",  # Specify a model available on OpenRouter\n",
    "    api_key=os.environ[\"QWEN3_API_KEY\"],\n",
    "    base_url=\"https://ellm.nrp-nautilus.io/v1\",\n",
    ")\n",
    "\n",
    "\n",
    "def grade_documents(\n",
    "    state: MessagesState,\n",
    ") -> Literal[\"generate_answer\", \"rewrite_question\"]:\n",
    "    \"\"\"Determine whether the retrieved documents are relevant to the question.\"\"\"\n",
    "    question = state[\"messages\"][0].content\n",
    "    context = state[\"messages\"][-1].content\n",
    "\n",
    "    prompt = GRADE_PROMPT.format(question=question, context=context)\n",
    "    response = (\n",
    "        grader_model\n",
    "        .with_structured_output(GradeDocuments).invoke(  \n",
    "            [{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "    )\n",
    "    score = response.binary_score\n",
    "\n",
    "    if score == \"yes\":\n",
    "        return \"generate_answer\"\n",
    "    else:\n",
    "        return \"rewrite_question\"\n",
    "from langchain_core.messages import convert_to_messages\n",
    "\n",
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What does Lilian Weng say about types of reward hacking?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"retrieve_blog_posts\",\n",
    "                        \"args\": {\"query\": \"types of reward hacking\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\"role\": \"tool\", \"content\": \"meow\", \"tool_call_id\": \"1\"},\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "grade_documents(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "51b1d4b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rewrite_question'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What does Lilian Weng say about types of reward hacking?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"retrieve_blog_posts\",\n",
    "                        \"args\": {\"query\": \"types of reward hacking\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": \"reward hacking can be categorized into two types: environment or goal misspecification, and reward tampering\",\n",
    "                \"tool_call_id\": \"1\",\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "grade_documents(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f31aea76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "REWRITE_PROMPT = (\n",
    "    \"Look at the input and try to reason about the underlying semantic intent / meaning.\\n\"\n",
    "    \"Here is the initial question:\"\n",
    "    \"\\n ------- \\n\"\n",
    "    \"{question}\"\n",
    "    \"\\n ------- \\n\"\n",
    "    \"Formulate an improved question:\"\n",
    ")\n",
    "\n",
    "\n",
    "response_model = init_chat_model(\n",
    "    \"gemma-3-4b-it\", # Use the specific model version you need\n",
    "    model_provider=\"google_genai\"\n",
    ")\n",
    "\n",
    "def rewrite_question(state: MessagesState):\n",
    "    \"\"\"Rewrite the original user question.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    prompt = REWRITE_PROMPT.format(question=question)\n",
    "    response = response_model.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
    "    return {\"messages\": [HumanMessage(content=response.content)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f49e2395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here's my reasoning about the initial question and a formulated improvement:\n",
      "\n",
      "**Reasoning about the Initial Question:**\n",
      "\n",
      "The initial question is quite broad. ‚ÄúTypes of reward hacking‚Äù is a specific topic within a larger area of AI ethics and alignment. It‚Äôs asking for a summary or categorization of different ways AI systems might be manipulated to receive rewards in ways that aren‚Äôt aligned with the intended goal.  It implies a desire to understand the *variety* of these behaviors, not just a single example.  It also suggests Lilian Weng has written about this topic.\n",
      "\n",
      "**Improved Question:**\n",
      "\n",
      "‚ÄúCan you summarize the different categories of reward hacking strategies that Lilian Weng has identified, and provide examples of each?‚Äù\n",
      "\n",
      "**Why this is an improvement:**\n",
      "\n",
      "*   **Specificity:** It asks for a *summary* and *categories*, forcing a more structured response.\n",
      "*   **Request for Examples:** Adding ‚Äúand provide examples‚Äù ensures the answer isn‚Äôt just theoretical.\n",
      "*   **Clearer Intent:** It directly asks for Lilian Weng‚Äôs specific insights, rather than a general discussion of reward hacking.\n",
      "\n",
      "Would you like me to refine this further, or perhaps consider a different angle for the question?\n"
     ]
    }
   ],
   "source": [
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What does Lilian Weng say about types of reward hacking?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"retrieve_blog_posts\",\n",
    "                        \"args\": {\"query\": \"types of reward hacking\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\"role\": \"tool\", \"content\": \"meow\", \"tool_call_id\": \"1\"},\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "\n",
    "response = rewrite_question(input)\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "adbc48b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE_PROMPT = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer the question. \"\n",
    "    \"If you don't know the answer, just say that you don't know. \"\n",
    "    \"Use three sentences maximum and keep the answer concise.\\n\"\n",
    "    \"Question: {question} \\n\"\n",
    "    \"Context: {context}\"\n",
    ")\n",
    "\n",
    "\n",
    "def generate_answer(state: MessagesState):\n",
    "    \"\"\"Generate an answer.\"\"\"\n",
    "    question = state[\"messages\"][0].content\n",
    "    context = state[\"messages\"][-1].content\n",
    "    prompt = GENERATE_PROMPT.format(question=question, context=context)\n",
    "    response = response_model.invoke([{\"role\": \"user\", \"content\": prompt}])\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "789ff859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Lilian Weng identifies two types of reward hacking: environment or goal misspecification, and reward tampering. These categories describe how the reward signal is manipulated or misrepresented to the agent. Essentially, it‚Äôs about exploiting flaws in the reward system itself.\n"
     ]
    }
   ],
   "source": [
    "input = {\n",
    "    \"messages\": convert_to_messages(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What does Lilian Weng say about types of reward hacking?\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"name\": \"retrieve_blog_posts\",\n",
    "                        \"args\": {\"query\": \"types of reward hacking\"},\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": \"reward hacking can be categorized into two types: environment or goal misspecification, and reward tampering\",\n",
    "                \"tool_call_id\": \"1\",\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "\n",
    "response = generate_answer(input)\n",
    "response[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ab52256a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# Define the nodes we will cycle between\n",
    "workflow.add_node(generate_query_or_respond)\n",
    "workflow.add_node(\"retrieve\", ToolNode([retriever_tool]))\n",
    "workflow.add_node(rewrite_question)\n",
    "workflow.add_node(generate_answer)\n",
    "\n",
    "workflow.add_edge(START, \"generate_query_or_respond\")\n",
    "\n",
    "# Decide whether to retrieve\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate_query_or_respond\",\n",
    "    # Assess LLM decision (call `retriever_tool` tool or respond to the user)\n",
    "    tools_condition,\n",
    "    {\n",
    "        # Translate the condition outputs to nodes in our graph\n",
    "        \"tools\": \"retrieve\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Edges taken after the `action` node is called.\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    # Assess agent decision\n",
    "    grade_documents,\n",
    ")\n",
    "workflow.add_edge(\"generate_answer\", END)\n",
    "workflow.add_edge(\"rewrite_question\", \"generate_query_or_respond\")\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c091073c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "84a1ae4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ChatGoogleGenerativeAIError",
     "evalue": "Error calling model 'gemma-3-4b-it' (INVALID_ARGUMENT): 400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'Function calling is not enabled for models/gemma-3-4b-it', 'status': 'INVALID_ARGUMENT'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mClientError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/RAG_Langchain_py311/lib/python3.11/site-packages/langchain_google_genai/chat_models.py:3047\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m   3046\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3047\u001b[39m     response: GenerateContentResponse = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3048\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3049\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3050\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ClientError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/RAG_Langchain_py311/lib/python3.11/site-packages/google/genai/models.py:5200\u001b[39m, in \u001b[36mModels.generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   5193\u001b[39m     logger.warning(\n\u001b[32m   5194\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mTools at indices [\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m] are not compatible with automatic function \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   5195\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mcalling (AFC). AFC is disabled. If AFC is intended, please \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   5198\u001b[39m         indices_str,\n\u001b[32m   5199\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m5200\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5201\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparsed_config\u001b[49m\n\u001b[32m   5202\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5204\u001b[39m remaining_remote_calls_afc = _extra_utils.get_max_remote_calls_afc(\n\u001b[32m   5205\u001b[39m     parsed_config\n\u001b[32m   5206\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/RAG_Langchain_py311/lib/python3.11/site-packages/google/genai/models.py:3997\u001b[39m, in \u001b[36mModels._generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   3995\u001b[39m request_dict = _common.encode_unserializable_types(request_dict)\n\u001b[32m-> \u001b[39m\u001b[32m3997\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3998\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[32m   3999\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4001\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[32m   4002\u001b[39m     config, \u001b[33m'\u001b[39m\u001b[33mshould_return_http_response\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4003\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/RAG_Langchain_py311/lib/python3.11/site-packages/google/genai/_api_client.py:1386\u001b[39m, in \u001b[36mBaseApiClient.request\u001b[39m\u001b[34m(self, http_method, path, request_dict, http_options)\u001b[39m\n\u001b[32m   1383\u001b[39m http_request = \u001b[38;5;28mself\u001b[39m._build_request(\n\u001b[32m   1384\u001b[39m     http_method, path, request_dict, http_options\n\u001b[32m   1385\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1386\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1387\u001b[39m response_body = (\n\u001b[32m   1388\u001b[39m     response.response_stream[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m response.response_stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1389\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/RAG_Langchain_py311/lib/python3.11/site-packages/google/genai/_api_client.py:1220\u001b[39m, in \u001b[36mBaseApiClient._request\u001b[39m\u001b[34m(self, http_request, http_options, stream)\u001b[39m\n\u001b[32m   1219\u001b[39m     retry = tenacity.Retrying(**retry_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1220\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_once\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[no-any-return]\u001b[39;00m\n\u001b[32m   1222\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retry(\u001b[38;5;28mself\u001b[39m._request_once, http_request, stream)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/RAG_Langchain_py311/lib/python3.11/site-packages/tenacity/__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/RAG_Langchain_py311/lib/python3.11/site-packages/tenacity/__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/RAG_Langchain_py311/lib/python3.11/site-packages/tenacity/__init__.py:400\u001b[39m, in \u001b[36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.iter_state.is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.retry_run_result):\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     \u001b[38;5;28mself\u001b[39m._add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutcome\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    401\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/RAG_Langchain_py311/lib/python3.11/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/RAG_Langchain_py311/lib/python3.11/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m     \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/RAG_Langchain_py311/lib/python3.11/site-packages/tenacity/__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/RAG_Langchain_py311/lib/python3.11/site-packages/google/genai/_api_client.py:1199\u001b[39m, in \u001b[36mBaseApiClient._request_once\u001b[39m\u001b[34m(self, http_request, stream)\u001b[39m\n\u001b[32m   1192\u001b[39m response = \u001b[38;5;28mself\u001b[39m._httpx_client.request(\n\u001b[32m   1193\u001b[39m     method=http_request.method,\n\u001b[32m   1194\u001b[39m     url=http_request.url,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1197\u001b[39m     timeout=http_request.timeout,\n\u001b[32m   1198\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1199\u001b[39m \u001b[43merrors\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAPIError\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[32m   1201\u001b[39m     response.headers, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response.text]\n\u001b[32m   1202\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/RAG_Langchain_py311/lib/python3.11/site-packages/google/genai/errors.py:121\u001b[39m, in \u001b[36mAPIError.raise_for_response\u001b[39m\u001b[34m(cls, response)\u001b[39m\n\u001b[32m    119\u001b[39m   response_json = response.body_segments[\u001b[32m0\u001b[39m].get(\u001b[33m'\u001b[39m\u001b[33merror\u001b[39m\u001b[33m'\u001b[39m, {})\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraise_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/RAG_Langchain_py311/lib/python3.11/site-packages/google/genai/errors.py:146\u001b[39m, in \u001b[36mAPIError.raise_error\u001b[39m\u001b[34m(cls, status_code, response_json, response)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m400\u001b[39m <= status_code < \u001b[32m500\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ClientError(status_code, response_json, response)\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[32m500\u001b[39m <= status_code < \u001b[32m600\u001b[39m:\n",
      "\u001b[31mClientError\u001b[39m: 400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'Function calling is not enabled for models/gemma-3-4b-it', 'status': 'INVALID_ARGUMENT'}}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mChatGoogleGenerativeAIError\u001b[39m               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat does Lilian Weng say about types of reward hacking?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mUpdate from node\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/RAG_Langchain_py311/lib/python3.11/site-packages/langgraph/pregel/main.py:2646\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2644\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2645\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2646\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2647\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2649\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2650\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2651\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2652\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2653\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2655\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2656\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/RAG_Langchain_py311/lib/python3.11/site-packages/langgraph/pregel/_runner.py:167\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    165\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/RAG_Langchain_py311/lib/python3.11/site-packages/langgraph/pregel/_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/RAG_Langchain_py311/lib/python3.11/site-packages/langgraph/_internal/_runnable.py:656\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    658\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/RAG_Langchain_py311/lib/python3.11/site-packages/langgraph/_internal/_runnable.py:400\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    398\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mgenerate_query_or_respond\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_query_or_respond\u001b[39m(state: MessagesState):\n\u001b[32m     17\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model to generate a response based on the current state. Given\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[33;03m    the question, it will decide to retrieve using the retriever tool, or simply respond to the user.\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     20\u001b[39m     response = (\n\u001b[32m     21\u001b[39m         \u001b[43mresponse_model\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mbind_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mretriever_tool\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[32m     23\u001b[39m     )\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [response]}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/RAG_Langchain_py311/lib/python3.11/site-packages/langchain_core/runnables/base.py:5557\u001b[39m, in \u001b[36mRunnableBindingBase.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5550\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5551\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   5552\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5555\u001b[39m     **kwargs: Any | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5556\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5557\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbound\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5558\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5559\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5560\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5561\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/RAG_Langchain_py311/lib/python3.11/site-packages/langchain_google_genai/chat_models.py:2535\u001b[39m, in \u001b[36mChatGoogleGenerativeAI.invoke\u001b[39m\u001b[34m(self, input, config, code_execution, stop, **kwargs)\u001b[39m\n\u001b[32m   2532\u001b[39m         msg = \u001b[33m\"\u001b[39m\u001b[33mTools are already defined.code_execution tool can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt be defined\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2533\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m-> \u001b[39m\u001b[32m2535\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/RAG_Langchain_py311/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:402\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    390\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    395\u001b[39m     **kwargs: Any,\n\u001b[32m    396\u001b[39m ) -> AIMessage:\n\u001b[32m    397\u001b[39m     config = ensure_config(config)\n\u001b[32m    398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    399\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    400\u001b[39m         cast(\n\u001b[32m    401\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    412\u001b[39m         ).message,\n\u001b[32m    413\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/RAG_Langchain_py311/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1121\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1112\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1114\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1118\u001b[39m     **kwargs: Any,\n\u001b[32m   1119\u001b[39m ) -> LLMResult:\n\u001b[32m   1120\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/RAG_Langchain_py311/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:931\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    930\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m931\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    937\u001b[39m         )\n\u001b[32m    938\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    939\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/RAG_Langchain_py311/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1233\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1231\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1232\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1233\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1237\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/RAG_Langchain_py311/lib/python3.11/site-packages/langchain_google_genai/chat_models.py:3051\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m   3047\u001b[39m     response: GenerateContentResponse = \u001b[38;5;28mself\u001b[39m.client.models.generate_content(\n\u001b[32m   3048\u001b[39m         **request,\n\u001b[32m   3049\u001b[39m     )\n\u001b[32m   3050\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ClientError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m3051\u001b[39m     \u001b[43m_handle_client_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3053\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/RAG_Langchain_py311/lib/python3.11/site-packages/langchain_google_genai/chat_models.py:145\u001b[39m, in \u001b[36m_handle_client_error\u001b[39m\u001b[34m(e, request)\u001b[39m\n\u001b[32m    143\u001b[39m model_name = request.get(\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33munknown\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    144\u001b[39m msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError calling model \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me.status\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m ChatGoogleGenerativeAIError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mChatGoogleGenerativeAIError\u001b[39m: Error calling model 'gemma-3-4b-it' (INVALID_ARGUMENT): 400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'Function calling is not enabled for models/gemma-3-4b-it', 'status': 'INVALID_ARGUMENT'}}",
      "During task with name 'generate_query_or_respond' and id 'd085e77c-039a-447b-f0f1-ffa9200bf3df'"
     ]
    }
   ],
   "source": [
    "for chunk in graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What does Lilian Weng say about types of reward hacking?\",\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "):\n",
    "    for node, update in chunk.items():\n",
    "        print(\"Update from node\", node)\n",
    "        update[\"messages\"][-1].pretty_print()\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4010a13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_Langchain_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
