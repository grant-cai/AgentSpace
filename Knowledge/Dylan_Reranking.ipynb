{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DexIGiwc4kTt"
      },
      "source": [
        "## Import Statements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "C1QIcG282Lda"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "from typing import List, Dict\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings.base import Embeddings\n",
        "from langchain_community.vectorstores import Neo4jVector\n",
        "from langchain_community.graphs import Neo4jGraph\n",
        "from huggingface_hub import login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QLHoQjg5ObZ"
      },
      "source": [
        "## Setup Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3EwM7wwR3WDd"
      },
      "outputs": [],
      "source": [
        "NEO4J_URI = os.environ.get(\"NEO4J_URI\")\n",
        "NEO4J_USERNAME = os.environ.get(\"NEO4J_USERNAME\")\n",
        "NEO4J_PASSWORD = os.environ.get(\"NEO4J_PASSWORD\")\n",
        "\n",
        "PDF_GLOB = \"/content/*.pdf\"\n",
        "INDEX_NAME = \"essay_chunk_agentspace\"\n",
        "\n",
        "EMBEDDING_MODEL_NAME = \"google/embeddinggemma-300m\"\n",
        "RERANKER_MODEL_NAME = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
        "\n",
        "CHUNK_SIZE = 500\n",
        "CHUNK_OVERLAP = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jVEVSt85am-"
      },
      "source": [
        "## Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ebdWHWAd3dM6"
      },
      "outputs": [],
      "source": [
        "class GemmaEmbeddings(Embeddings):\n",
        "    def __init__(self, model_name: str):\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "\n",
        "    def _normalize(self, v):\n",
        "        norm = np.linalg.norm(v)\n",
        "        return v / norm if norm > 0 else v\n",
        "\n",
        "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
        "        vectors = self.model.encode(texts, convert_to_numpy=True)\n",
        "        return [self._normalize(v).tolist() for v in vectors]\n",
        "\n",
        "    def embed_query(self, text: str) -> List[float]:\n",
        "        v = self.model.encode([text], convert_to_numpy=True)[0]\n",
        "        return self._normalize(v).tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPr_2DP75dnm"
      },
      "source": [
        "## Data Ingestion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AdyaGBN53fyx"
      },
      "outputs": [],
      "source": [
        "def load_pdfs(path_glob: str):\n",
        "    docs = []\n",
        "    for file in glob.glob(path_glob):\n",
        "        loader = PyPDFLoader(file)\n",
        "        docs.extend(loader.load())\n",
        "    return docs\n",
        "\n",
        "\n",
        "def split_documents(docs):\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=CHUNK_SIZE,\n",
        "        chunk_overlap=CHUNK_OVERLAP\n",
        "    )\n",
        "    chunks = splitter.split_documents(docs)\n",
        "\n",
        "    for i, c in enumerate(chunks):\n",
        "        c.metadata[\"chunk_id\"] = i\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGdtUAq65hnO"
      },
      "source": [
        "## Vector Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "E6OGTc-63idh"
      },
      "outputs": [],
      "source": [
        "def build_vectorstore(chunks, embeddings):\n",
        "    return Neo4jVector.from_documents(\n",
        "        documents=chunks,\n",
        "        embedding=embeddings,\n",
        "        url=NEO4J_URI,\n",
        "        username=NEO4J_USERNAME,\n",
        "        password=NEO4J_PASSWORD,\n",
        "        index_name=INDEX_NAME,\n",
        "        node_label=\"Chunk\",\n",
        "        text_node_property=\"text\",\n",
        "        embedding_node_property=\"embedding\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1B6PYOu5kWD"
      },
      "source": [
        "## Graph Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "M2urmYFA3ltw"
      },
      "outputs": [],
      "source": [
        "def build_graph_relationships(chunks):\n",
        "    graph = Neo4jGraph(\n",
        "        url=NEO4J_URI,\n",
        "        username=NEO4J_USERNAME,\n",
        "        password=NEO4J_PASSWORD\n",
        "    )\n",
        "\n",
        "    by_source: Dict[str, List] = {}\n",
        "    for c in chunks:\n",
        "        src = c.metadata.get(\"source\", \"unknown\")\n",
        "        by_source.setdefault(src, []).append(c)\n",
        "\n",
        "    for source, source_chunks in by_source.items():\n",
        "        source_chunks.sort(key=lambda x: x.metadata[\"chunk_id\"])\n",
        "\n",
        "        graph.query(\n",
        "            \"MERGE (d:Document {name: $name}) SET d.chunk_count = $n\",\n",
        "            {\"name\": source, \"n\": len(source_chunks)}\n",
        "        )\n",
        "\n",
        "        for i, c in enumerate(source_chunks):\n",
        "            graph.query(\n",
        "                \"\"\"\n",
        "                MATCH (d:Document {name: $source})\n",
        "                MATCH (c:Chunk {chunk_id: $cid})\n",
        "                MERGE (c)-[:PART_OF]->(d)\n",
        "                \"\"\",\n",
        "                {\"source\": source, \"cid\": c.metadata[\"chunk_id\"]}\n",
        "            )\n",
        "\n",
        "            if i < len(source_chunks) - 1:\n",
        "                graph.query(\n",
        "                    \"\"\"\n",
        "                    MATCH (c1:Chunk {chunk_id: $c1})\n",
        "                    MATCH (c2:Chunk {chunk_id: $c2})\n",
        "                    MERGE (c1)-[:NEXT]->(c2)\n",
        "                    \"\"\",\n",
        "                    {\n",
        "                        \"c1\": c.metadata[\"chunk_id\"],\n",
        "                        \"c2\": source_chunks[i + 1].metadata[\"chunk_id\"],\n",
        "                    },\n",
        "                )\n",
        "\n",
        "    return graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Op_ucwgs5qL0"
      },
      "source": [
        "## Retrieval and Reranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Y_lxISM93qtB"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import CrossEncoder\n",
        "import numpy as np\n",
        "\n",
        "class HybridRetriever:\n",
        "    def __init__(\n",
        "        self,\n",
        "        vectorstore,\n",
        "        graph,\n",
        "        reranker_model: str,\n",
        "        mmr_lambda: float = 0.5,\n",
        "        strong_score_threshold: float = 0.2,\n",
        "    ):\n",
        "        self.vectorstore = vectorstore\n",
        "        self.graph = graph\n",
        "        self.reranker = CrossEncoder(reranker_model)\n",
        "        self.mmr_lambda = mmr_lambda\n",
        "        self.strong_score_threshold = strong_score_threshold\n",
        "\n",
        "    def _get_context(self, chunk_id: int, k: int):\n",
        "        if k == 0:\n",
        "            return None\n",
        "\n",
        "        query = f\"\"\"\n",
        "        MATCH (c:Chunk {{chunk_id: $cid}})\n",
        "        OPTIONAL MATCH (b:Chunk)-[:NEXT*1..{k}]->(c)\n",
        "        OPTIONAL MATCH (c)-[:NEXT*1..{k}]->(a:Chunk)\n",
        "        RETURN c.text AS text,\n",
        "               [x IN collect(DISTINCT b) | x.text] AS before,\n",
        "               [x IN collect(DISTINCT a) | x.text] AS after\n",
        "        \"\"\"\n",
        "        res = self.graph.query(query, {\"cid\": chunk_id})\n",
        "        return res[0] if res else None\n",
        "\n",
        "    def _mmr(self, query_embedding, doc_embeddings, docs, k):\n",
        "        selected = []\n",
        "        selected_embeddings = []\n",
        "\n",
        "        for _ in range(k):\n",
        "            scores = []\n",
        "            for i, emb in enumerate(doc_embeddings):\n",
        "                if docs[i] in selected:\n",
        "                    continue\n",
        "\n",
        "                relevance = np.dot(query_embedding, emb)\n",
        "                diversity = 0\n",
        "                if selected_embeddings:\n",
        "                    diversity = max(\n",
        "                        np.dot(emb, s_emb) for s_emb in selected_embeddings\n",
        "                    )\n",
        "\n",
        "                mmr_score = (\n",
        "                    self.mmr_lambda * relevance\n",
        "                    - (1 - self.mmr_lambda) * diversity\n",
        "                )\n",
        "                scores.append((mmr_score, i))\n",
        "\n",
        "            if not scores:\n",
        "                break\n",
        "\n",
        "            _, idx = max(scores, key=lambda x: x[0])\n",
        "            selected.append(docs[idx])\n",
        "            selected_embeddings.append(doc_embeddings[idx])\n",
        "\n",
        "        return selected\n",
        "\n",
        "    # Troubleshoot document aware implementation\n",
        "    def _mmr_document_aware(self, query_embedding, doc_embeddings, docs, k):\n",
        "        selected = []\n",
        "        selected_embeddings = []\n",
        "        selected_docs = set()\n",
        "\n",
        "        for _ in range(k):\n",
        "            scores = []\n",
        "            for i, emb in enumerate(doc_embeddings):\n",
        "                doc_id = docs[i].metadata.get(\"source\")\n",
        "                if docs[i] in selected:\n",
        "                    continue\n",
        "\n",
        "                relevance = np.dot(query_embedding, emb)\n",
        "\n",
        "                diversity = 0\n",
        "                if selected_embeddings:\n",
        "                    diversity = max(np.dot(emb, s_emb) for s_emb in selected_embeddings)\n",
        "\n",
        "                doc_penalty = 0.5 if doc_id in selected_docs else 0.0\n",
        "\n",
        "                mmr_score = (\n",
        "                    self.mmr_lambda * relevance\n",
        "                    - (1 - self.mmr_lambda) * diversity\n",
        "                    - doc_penalty\n",
        "                )\n",
        "                scores.append((mmr_score, i))\n",
        "\n",
        "            if not scores:\n",
        "                break\n",
        "\n",
        "            _, idx = max(scores, key=lambda x: x[0])\n",
        "            selected.append(docs[idx])\n",
        "            selected_embeddings.append(doc_embeddings[idx])\n",
        "            selected_docs.add(docs[idx].metadata.get(\"source\"))\n",
        "\n",
        "        return selected\n",
        "\n",
        "    def retrieve(self, query: str, top_k: int = 20, rerank_k: int = 6):\n",
        "        candidates = self.vectorstore.similarity_search(query, k=top_k)\n",
        "\n",
        "        query_emb = self.vectorstore.embedding.embed_query(query)\n",
        "        doc_embs = [\n",
        "            self.vectorstore.embedding.embed_query(c.page_content)\n",
        "            for c in candidates\n",
        "        ]\n",
        "\n",
        "        mmr_candidates = self._mmr(\n",
        "            query_embedding=query_emb,\n",
        "            doc_embeddings=doc_embs,\n",
        "            docs=candidates,\n",
        "            k=rerank_k * 2,\n",
        "        )\n",
        "\n",
        "        rerank_inputs = []\n",
        "        contexts = {}\n",
        "\n",
        "        for c in mmr_candidates:\n",
        "            cid = c.metadata.get(\"chunk_id\")\n",
        "            context = self._get_context(cid, k=1)\n",
        "            contexts[cid] = context\n",
        "\n",
        "            combined_text = c.page_content\n",
        "            if context:\n",
        "                combined_text = \" \".join(\n",
        "                    context.get(\"before\", [])\n",
        "                    + [c.page_content]\n",
        "                    + context.get(\"after\", [])\n",
        "                )\n",
        "\n",
        "            rerank_inputs.append([query, combined_text])\n",
        "\n",
        "        scores = self.reranker.predict(rerank_inputs)\n",
        "\n",
        "        reranked = sorted(\n",
        "            zip(mmr_candidates, scores),\n",
        "            key=lambda x: x[1],\n",
        "            reverse=True,\n",
        "        )[:rerank_k]\n",
        "\n",
        "        results = []\n",
        "        for doc, score in reranked:\n",
        "            cid = doc.metadata.get(\"chunk_id\")\n",
        "\n",
        "            context_k = 2 if score >= self.strong_score_threshold else 0\n",
        "            context = self._get_context(cid, k=context_k)\n",
        "\n",
        "            results.append({\n",
        "                \"text\": doc.page_content,\n",
        "                \"score\": float(score),\n",
        "                \"source\": doc.metadata.get(\"source\"),\n",
        "                \"chunk_id\": cid,\n",
        "                \"context\": context,\n",
        "            })\n",
        "\n",
        "        return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hjB2sAT5ubx"
      },
      "source": [
        "## Usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 864,
          "referenced_widgets": [
            "16c59ef6ce0247d98b4156729a5a1b5a",
            "07e3150d83fc44448782d70cc9cbd2fc",
            "e83af5997e374a9f9712bfebb8b40828",
            "9af7c70fb8774d9e8b3add1e3c432de6",
            "d384c74d36744e33a1883186f4303e37",
            "5e80df8f84a34673b751943125b81186",
            "38c4c089034d43a18c321a162b13c210",
            "a93a8ca405cb4ce48e8178593fdb2dad",
            "56cc5ad693e947e4982c8cc568d6bbbf",
            "bee8713347f148ba88788a6a6dea8db7",
            "007822c7ecc242728a50b417debec930",
            "d52c90d248164ba090ddfd86e7da1bf3",
            "734d435efe28451f9b2a9be01ed56530",
            "9383a9e884f24d7ebbd02e4410c29ab4",
            "62da43bb9b4648c985b4a899131e683a",
            "30e10d05afbf4122822467d114107df5",
            "08e74ff522f744f9bb4367d3163658de",
            "5454b2c076cc4067af76864fa2019281",
            "43d22f5c0b0c44a5ad5d46ee879c66b1",
            "4396ef18e00946519437e02d9354068e",
            "2fc3b84f41c84528a1076b7040199870",
            "c44acafdafe14e94946a283f531cbbd7"
          ]
        },
        "id": "pFFQVqoo3O1C",
        "outputId": "5800e78b-8edc-4232-8c33-9f57540306d6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "16c59ef6ce0247d98b4156729a5a1b5a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/314 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3530743014.py:2: LangChainDeprecationWarning: The class `Neo4jGraph` was deprecated in LangChain 0.3.8 and will be removed in 1.0. An updated version of the class exists in the `langchain-neo4j package and should be used instead. To use it run `pip install -U `langchain-neo4j` and import as `from `langchain_neo4j import Neo4jGraph``.\n",
            "  graph = Neo4jGraph(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d52c90d248164ba090ddfd86e7da1bf3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/105 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "BertForSequenceClassification LOAD REPORT from: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
            "Key                          | Status     |  | \n",
            "-----------------------------+------------+--+-\n",
            "bert.embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Result 1 (score=5.360) ---\n",
            "means of learning and improving his writing. However, the mediums through which he was able \n",
            "to practice were severely limited in comparison to other white people his age, which is clearly \n",
            "highlighted through the contrast he uses between the conventional items people used to learn and \n",
            "what he actu\n",
            "\n",
            "--- Result 2 (score=3.981) ---\n",
            "determination. He then used this skill to doctor letters from masters as excusals, eventually \n",
            "earning his freedom and writing his own slave narrative, Narrative of the Life of Frederick \n",
            "Douglass. The way Franklin and Douglass learned to write and how they utilized their skills in \n",
            "writing demonstr\n",
            "\n",
            "--- Result 3 (score=3.700) ---\n",
            "similarities end there; Douglass does not hide his hand to get his work published and seen; he \n",
            "does so in order for his work to go unnoticed. The content of the work is also drastically \n",
            "different, with Franklin writing more long form content to express the nuances of his writing \n",
            "style and display\n",
            "\n",
            "--- Result 4 (score=3.588) ---\n",
            "skill of writing is critical, for without it he would not even be able to come close to doctoring a \n",
            "letter from his master. For Douglass, writing is not just a means of freedom but also plays a \n",
            "critical role in life or death; if one did not create these fake letters correctly, they would be easily\n",
            "\n",
            "--- Result 5 (score=3.282) ---\n",
            "as an addition to an already solid foundation of English knowledge. Douglass, on the other hand, \n",
            "learned and improved his writing by copying letters from men at the shipyards and learning from \n",
            "boys he knew were literate; “[his] copy-book was the board fence, brick wall, and pavement; \n",
            "[his] pen an\n",
            "\n",
            "--- Result 6 (score=3.272) ---\n",
            "an empowerment of his ability. Still though, Douglass had to learn the alphabet from his white \n",
            "master, and no matter how kind she initially was to him, this block along racial lines defines an \n",
            "inhibitor on the education and capabilities of African American enslaved people at the time. \n",
            "Sophia’s hu\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    docs = load_pdfs(PDF_GLOB)\n",
        "    chunks = split_documents(docs)\n",
        "\n",
        "    embeddings = GemmaEmbeddings(EMBEDDING_MODEL_NAME)\n",
        "    vectorstore = build_vectorstore(chunks, embeddings)\n",
        "    graph = build_graph_relationships(chunks)\n",
        "\n",
        "    retriever = HybridRetriever(vectorstore, graph, RERANKER_MODEL_NAME)\n",
        "\n",
        "    query = \"How did Douglass refine his writing?\"\n",
        "    results = retriever.retrieve(query)\n",
        "\n",
        "    for i, r in enumerate(results, 1):\n",
        "        print(f\"\\n--- Result {i} (score={r['score']:.3f}) ---\")\n",
        "        print(r[\"text\"][:300])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "DexIGiwc4kTt"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
